/**
 * Universal player for audio sprites generated by the 'audiosprite' tool.
 * Requires an AudioContext and fetch to be injected.
 * Uses AudioBufferQueueSourceNode and buffer splitting for mobile stability.
 */
export class AudioSpritePlayer {
  audioContext: any | null;
  fetch: any | null;
  audioBuffer: any | null; // The full audio buffer
  manifest: any | null;
  platform: string;
  // Cache for the small, pre-split AudioBuffers used by mobile's QueueSourceNode
  private spriteBufferCache: Record<string, any> = {};

  constructor({
    audioContext,
    fetch,
    platform,
  }: {
    audioContext: any | null;
    fetch: any;
    platform: string;
  }) {
    if (!audioContext) {
      if (platform === 'web') {
        // Web doesnt need to provide AudioContext
        // @ts-ignore
        this.audioContext = new AudioContext();
      } else {
        throw new Error(
          'An AudioContext instance must be provided from react-native-audio-api.'
        );
      }
    }
    if (!fetch) {
      throw new Error('A fetch implementation must be provided.');
    }
    this.audioContext = audioContext;
    this.fetch = fetch;
    this.audioBuffer = null;
    this.manifest = null;
    this.platform = platform; // 'web', 'ios', 'android', etc.
    if (
      this.audioContext?.createBufferSource?.constructor?.name ===
      'AsyncFunction'
    ) {
      console.log(
        'createBufferSource is async! going with web default AudioContext'
      );
      // Can be removed after this PR gets merged
      //https://github.com/software-mansion/react-native-audio-api/issues/574
      // @ts-ignore
      this.audioContext = new AudioContext();
    }
  }

  /**
   * Caches pre-split AudioBuffers for each sprite, which is necessary
   * for stable playback on mobile using the BufferQueueSourceNode.
   */
  private _cacheSpriteBuffers() {
    if (!this.audioBuffer || !this.manifest || this.platform === 'web') {
      return; // Only necessary for mobile platforms
    }

    const sampleRate = this.audioBuffer.sampleRate;
    const numChannels = this.audioBuffer.numberOfChannels;
    this.spriteBufferCache = {};

    for (const soundName in this.manifest.spritemap) {
      const sound = this.manifest.spritemap[soundName];

      // Calculate frame indices
      const startFrame = Math.floor(sound.start * sampleRate);
      const endFrame = Math.ceil(sound.end * sampleRate);
      const durationFrames = endFrame - startFrame;

      if (durationFrames <= 0) {
        console.warn(
          `Sprite "${soundName}" has zero or negative duration. Skipping.`
        );
        continue;
      }

      // 1. Create a new empty buffer for the sprite
      const spriteBuffer = this.audioContext.createBuffer(
        numChannels,
        durationFrames,
        sampleRate
      );

      // 2. Copy data from the full buffer to the new sprite buffer
      for (let i = 0; i < numChannels; i++) {
        const sourceData = this.audioBuffer.getChannelData(i);
        const destinationData = spriteBuffer.getChannelData(i);

        // Extract the segment data (efficient array copy)
        const segment = sourceData.subarray(startFrame, endFrame);
        destinationData.set(segment);
      }

      this.spriteBufferCache[soundName] = spriteBuffer;
      // console.log(`Cached sprite buffer for ${soundName}, frames: ${durationFrames}`);
    }
  }

  async load(json: any, audio?: any) {
    try {
      let decodedBuffer: any;

      // --- Fetching and Decoding Logic (Uses existing logic) ---
      if (typeof json === 'string') {
        const response = await this.fetch(json);
        if (!response.ok) {
          throw new Error(`Failed to fetch manifest: ${response.statusText}`);
        }
        this.manifest = await response.json();

        if (!this.manifest.resources || !this.manifest.spritemap) {
          throw new Error(
            'Invalid audiosprite manifest format. Missing "resources" or "spritemap".'
          );
        }

        const audioFileName = this.manifest.resources[0];
        const audioUrl = new URL(audioFileName, response.url).href;

        const audioResponse = await this.fetch(audioUrl);
        if (!audioResponse.ok) {
          throw new Error(
            `Failed to fetch audio file: ${audioResponse.statusText}`
          );
        }

        const arrayBuffer = await audioResponse.arrayBuffer();
        decodedBuffer = await this.audioContext.decodeAudioData(arrayBuffer);
      } else {
        this.manifest = json;
        if (!this.manifest.resources || !this.manifest.spritemap) {
          throw new Error(
            'Invalid audiosprite manifest format. Missing "resources" or "spritemap".'
          );
        }

        let arrayBuffer;
        if (typeof audio === 'string') {
          const audioResponse = await this.fetch(audio);
          if (!audioResponse.ok) {
            throw new Error(
              `Failed to fetch audio file: ${audioResponse.statusText}`
            );
          }
          arrayBuffer = await audioResponse.arrayBuffer();
        } else {
          arrayBuffer = audio;
        }
        decodedBuffer = await this.audioContext.decodeAudioData(arrayBuffer);
      }
      // --- End Fetching and Decoding Logic ---

      this.audioBuffer = decodedBuffer;

      // ðŸš¨ CRITICAL: Split and cache buffers for mobile stability/correctness
      this._cacheSpriteBuffers();

      console.log('RNAS: Audio sprite loaded successfully.');
    } catch (error) {
      console.error('Failed to load audio sprite:', error);
      throw error; // Re-throw for user to catch
    }
  }

  play(soundName: string) {
    if (!this.audioBuffer || !this.manifest) {
      console.warn('Audio sprite not loaded. Call load() first.');
      return;
    }

    // Resume context if it was suspended (must be non-blocking here)
    if (this.audioContext.state === 'suspended') {
      this.audioContext.resume().catch((e: any) => {
        console.error('Failed to resume AudioContext:', e);
      });
    }

    const sound = this.manifest.spritemap[soundName];
    if (!sound) {
      console.warn(`Sound "${soundName}" not found in spritemap.`);
      return;
    }

    const duration = sound.end - sound.start;
    if (duration <= 0) {
      console.warn(`Sound "${soundName}" has invalid duration.`);
      return;
    }

    let source: any;

    // ðŸš¨ MOBILE LOGIC: Use AudioBufferQueueSourceNode with cached split buffer
    if (this.platform !== 'web') {
      const spriteBuffer = this.spriteBufferCache[soundName];

      if (!spriteBuffer) {
        console.error(
          `RNAS Error: Split buffer for "${soundName}" not found in cache.`
        );
        return;
      }

      if (!this.audioContext.createBufferQueueSource) {
        console.error(
          'RNAS Error: createBufferQueueSource is not available on this native platform.'
        );
        return;
      }

      source = this.audioContext.createBufferQueueSource();

      // Mobile Implementation: Enqueue the specific, short sprite buffer
      source.enqueueBuffer(spriteBuffer);

      source.connect(this.audioContext.destination);

      // This will play the short buffer from its start to its end.
      source.start(1);
    } else {
      // ðŸŒ WEB LOGIC (Standard Web Audio API)
      source = this.audioContext.createBufferSource();

      if (!source || typeof source.connect !== 'function') {
        console.error(
          'RNAS Error: createBufferSource() returned an invalid object on web. Aborting playback.'
        );
        return;
      }

      source.buffer = this.audioBuffer;
      source.connect(this.audioContext.destination);

      // Use the 'audiosprite' format: start(when, offset, duration)
      source.start(
        0, // Start playing now
        sound.start, // The offset
        duration // The calculated duration
      );
    }

    console.log(`RNAS: played ${soundName} on ${this.platform}`);
  }

  getManifest() {
    return this.manifest;
  }

  getAudioBuffer() {
    return this.audioBuffer;
  }
}
