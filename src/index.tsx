/**
 * Universal player for audio sprites generated by the 'audiosprite' tool.
 * Requires an AudioContext and fetch to be injected.
 * Uses AudioBufferQueueSourceNode and buffer splitting for mobile stability.
 */
export class AudioSpritePlayer {
  audioContext: any | null;
  fetch: any | null;
  audioBuffer: any | null; // The full audio buffer
  manifest: any | null;
  platform: string;
  // Cache for the small, pre-split AudioBuffers used by mobile's QueueSourceNode
  private spriteBufferCache: Record<string, any> = {};
  private loopingSource: any | null = null;

  constructor({
    audioContext,
    fetch,
    platform,
  }: {
    audioContext: any | null;
    fetch: any;
    platform: string;
  }) {
    if (!audioContext) {
      if (platform === 'web') {
        // Web doesnt need to provide AudioContext
        // @ts-ignore
        this.audioContext = new AudioContext();
      } else {
        throw new Error(
          'An AudioContext instance must be provided from react-native-audio-api.'
        );
      }
    }
    if (!fetch) {
      throw new Error('A fetch implementation must be provided.');
    }
    this.audioContext = audioContext;
    this.fetch = fetch;
    this.audioBuffer = null;
    this.manifest = null;
    this.platform = platform; // 'web', 'ios', 'android', etc.
    if (
      this.audioContext?.createBufferSource?.constructor?.name ===
      'AsyncFunction'
    ) {
      console.log(
        'createBufferSource is async! going with web default AudioContext'
      );
      // Can be removed after this PR gets merged
      //https://github.com/software-mansion/react-native-audio-api/issues/574
      // @ts-ignore
      this.audioContext = new AudioContext();
    }
  }

  /**
   * Caches pre-split AudioBuffers for each sprite, which is necessary
   * for stable playback on mobile using the BufferQueueSourceNode.
   *
   * This method iterates through the audio sprite manifest and creates a separate
   * AudioBuffer for each sound sprite. These smaller buffers are then stored
   * in `this.spriteBufferCache` for efficient playback on mobile platforms,
   * especially when using `AudioBufferQueueSourceNode`.
   *
   * The `audiosprite` manifest is expected to have the following structure for each sprite:
   * `[start_time_ms, duration_ms, loop_boolean (optional)]`
   *
   * @example
   * // Example audiosprite manifest structure:
   * {
   *   "urls": ["audio.mp3", "audio.ogg"],
   *   "sprite": {
   *     "sound1": [0, 1000, false],       // start at 0ms, duration 1000ms, no loop
   *     "sound2": [1500, 500, true],      // start at 1500ms, duration 500ms, loop
   *     "background": [2000, 30000, true] // start at 2000ms, duration 30000ms, loop
   *   }
   * }
   */
  private _cacheSpriteBuffers() {
    if (!this.audioBuffer || !this.manifest) {
      return; // Only necessary for mobile platforms
    }

    const sampleRate = this.audioBuffer.sampleRate;
    const numChannels = this.audioBuffer.numberOfChannels;
    this.spriteBufferCache = {};

    for (const soundName in this.manifest.sprite) {
      const sound = this.manifest.sprite[soundName];

      // Calculate frame indices based on audiosprite format: [start, duration, loop]
      const startFrame = Math.floor((sound[0] * sampleRate) / 1000); // Convert ms to frames
      const endFrame = Math.ceil(((sound[0] + sound[1]) * sampleRate) / 1000); // Convert ms to frames
      const durationFrames = endFrame - startFrame;

      if (durationFrames <= 0) {
        console.warn(
          `Sprite "${soundName}" has zero or negative duration. Skipping.`
        );
        continue;
      }

      // 1. Create a new empty buffer for the sprite
      const spriteBuffer = this.audioContext.createBuffer(
        numChannels,
        durationFrames,
        sampleRate
      );

      // 2. Copy data from the full buffer to the new sprite buffer
      for (let i = 0; i < numChannels; i++) {
        const sourceData = this.audioBuffer.getChannelData(i);
        const destinationData = spriteBuffer.getChannelData(i);

        // Extract the segment data (efficient array copy)
        const segment = sourceData.subarray(startFrame, endFrame);
        destinationData.set(segment);
      }

      this.spriteBufferCache[soundName] = spriteBuffer;
      // console.log(`Cached sprite buffer for ${soundName}, frames: ${durationFrames}`);
    }
  }

  async load(json: any, audio?: any) {
    try {
      let decodedBuffer: any;

      // --- Fetching and Decoding Logic (Uses existing logic) ---
      if (typeof json === 'string') {
        const response = await this.fetch(json);
        if (!response.ok) {
          throw new Error(`Failed to fetch manifest: ${response.statusText}`);
        }
        this.manifest = await response.json();

        if (!this.manifest.urls || !this.manifest.sprite) {
          throw new Error(
            'Invalid audiosprite manifest format. Missing "urls" or "sprite".'
          );
        }

        const audioFileName = this.manifest.urls[0];
        const audioUrl = new URL(audioFileName, response.url).href;

        const audioResponse = await this.fetch(audioUrl);
        if (!audioResponse.ok) {
          throw new Error(
            `Failed to fetch audio file: ${audioResponse.statusText}`
          );
        }

        const arrayBuffer = await audioResponse.arrayBuffer();
        decodedBuffer = await this.audioContext.decodeAudioData(arrayBuffer);
      } else {
        this.manifest = json;
        if (!this.manifest.urls || !this.manifest.sprite) {
          throw new Error(
            'Invalid audiosprite manifest format. Missing "urls" or "sprite".'
          );
        }

        let arrayBuffer;
        if (typeof audio === 'string') {
          const audioResponse = await this.fetch(audio);
          if (!audioResponse.ok) {
            throw new Error(
              `Failed to fetch audio file: ${audioResponse.statusText}`
            );
          }
          arrayBuffer = await audioResponse.arrayBuffer();
        } else {
          arrayBuffer = audio;
        }
        decodedBuffer = await this.audioContext.decodeAudioData(arrayBuffer);
      }
      // --- End Fetching and Decoding Logic ---

      this.audioBuffer = decodedBuffer;

      // ðŸš¨ CRITICAL: Split and cache buffers for mobile stability/correctness
      this._cacheSpriteBuffers();

      console.log('RNAS: Audio sprite loaded successfully.');
    } catch (error) {
      console.error('Failed to load audio sprite:', error);
      throw error; // Re-throw for user to catch
    }
  }

  play(soundName: string) {
    if (!this.audioBuffer || !this.manifest) {
      console.warn('Audio sprite not loaded. Call load() first.');
      return;
    }

    // Resume context if it was suspended (must be non-blocking here)
    if (this.audioContext.state === 'suspended') {
      this.audioContext.resume().catch((e: any) => {
        console.error('Failed to resume AudioContext:', e);
      });
    }

    const sound = this.manifest.sprite[soundName];
    if (!sound) {
      console.warn(`Sound "${soundName}" not found in spritemap.`);
      return;
    }

    const duration = sound[1];
    if (duration <= 0) {
      console.warn(`Sound "${soundName}" has invalid duration.`);
      return;
    }

    let source: any;
    const spriteBuffer = this.spriteBufferCache[soundName];

    // ðŸš¨ MOBILE LOGIC: Use AudioBufferQueueSourceNode with cached split buffer
    if (this.platform !== 'web') {
      if (!spriteBuffer) {
        console.error(
          `RNAS Error: Split buffer for "${soundName}" not found in cache.`
        );
        return;
      }

      if (!this.audioContext.createBufferQueueSource) {
        console.error(
          'RNAS Error: createBufferQueueSource is not available on this native platform.'
        );
        return;
      }

      const loop = sound[2];

      if (loop) {
        // Always use AudioBufferQueueSourceNode
        source = this.audioContext.createBufferQueueSource();
        source.enqueueBuffer(spriteBuffer);
        // run a while loop until stop()
        source.loop = true;
        source.loopStart = sound[0] / 1000; // Convert ms to seconds
        source.loopEnd = (sound[0] + sound[1]) / 1000; // Convert ms to seconds
        source.connect(this.audioContext.destination);
        source.start(0); // Start immediately
        this.loopingSource = source; // Store reference to looping source
      } else {
        // For non-looping sounds on mobile, use AudioBufferQueueSourceNode
        source = this.audioContext.createBufferQueueSource();
        source.enqueueBuffer(spriteBuffer);
        source.connect(this.audioContext.destination);
        source.start(1);
        console.log('non loop', soundName);
      }
    } else {
      // ðŸŒ WEB LOGIC (Standard Web Audio API)
      source = this.audioContext.createBufferSource();

      if (!source || typeof source.connect !== 'function') {
        console.error(
          'RNAS Error: createBufferSource() returned an invalid object on web. Aborting playback.'
        );
        return;
      }

      source.buffer = spriteBuffer;
      source.connect(this.audioContext.destination);

      const loop = sound[2]; // audiosprite stores loop as the third element in the array
      if (loop) {
        source.loop = true;
        source.loopStart = 0; // Relative to the spriteBuffer
        source.loopEnd = sound[1] / 1000; // Duration of the spriteBuffer
        source.start(0); // Start immediately, no offset for the individual spriteBuffer
        this.loopingSource = source; // Store reference to looping source
      } else {
        // Use the 'audiosprite' format: start(when, offset, duration)
        source.start(
          0, // Start playing now
          0, // The offset in seconds (relative to the spriteBuffer)
          sound[1] / 1000 // The calculated duration in seconds
        );
      }
    }

    console.log(`RNAS: played ${soundName} on ${this.platform}`);
  }

  getManifest() {
    return this.manifest;
  }

  getAudioBuffer() {
    return this.audioBuffer;
  }

  /**
   * Stops the currently looping audio sprite.
   * If a looping sound is playing, it will be stopped immediately.
   */
  stop() {
    if (this.loopingSource) {
      this.loopingSource.stop();
      this.loopingSource = null;
      console.log('RNAS: Looping audio stopped.');
    } else {
      console.log('RNAS: No looping audio to stop.');
    }
  }
}
